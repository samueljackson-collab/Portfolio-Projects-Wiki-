"""
Claude Opus Reviewer
====================
Takes drafts generated by the free AI agent and runs them through
Claude Opus (or better) for quality review and improvement.

This is the final quality gate before entries go live.
The reviewer:
1. Validates technical accuracy
2. Improves prose quality and consistency
3. Ensures style matches existing entries
4. Catches hallucinations or factual errors
5. Polishes code examples
"""

import json
import logging
import os
import re
import time
import urllib.error
import urllib.parse
import urllib.request
from typing import Optional

logger = logging.getLogger(__name__)


class OpusReviewer:
    """Reviews and improves AI-generated wiki drafts using Claude Opus."""

    REVIEW_SYSTEM_PROMPT = """You are a senior technical editor reviewing wiki entries for a software engineering portfolio. Your job is to take a draft entry (generated by another AI) and improve it to production quality.

REVIEW CHECKLIST:
1. **Technical Accuracy**: Flag and fix any technologies, patterns, or claims that seem wrong or hallucinated. If you can't verify something, remove it.
2. **Prose Quality**: Tighten descriptions. Remove fluff. Every sentence should add value.
3. **Style Consistency**: Match the tone and structure of the example entries provided.
4. **Code Examples**: Ensure all code is syntactically correct and idiomatic for its language. Fix any issues.
5. **Completeness**: Ensure all required fields are present and substantive.
6. **STRIDE Threat Model**: If present, ensure all 6 categories are covered with specific, actionable mitigations (not generic advice).
7. **ADR Format**: If present, ensure it follows: Status, Context, Decision, Consequences format.
8. **Tags/Technologies**: Ensure proper formatting (tags: lowercase-hyphenated, techs: properly capitalized).

IMPORTANT:
- Return the IMPROVED version of the entry, not a review commentary.
- If the draft is already high quality, return it with minimal changes.
- Do NOT add information you can't verify from the provided context.
- Do NOT change the fundamental structure, only improve content quality.
- Preserve all factual information from the original draft."""

    REVIEW_USER_TEMPLATE = """Review and improve this wiki entry draft.

## Draft Entry
```json
{draft_json}
```

## Change Type
{change_type}

## Original Repository Context
- **Repository:** {repo_name}
- **Description:** {repo_description}
- **Languages:** {languages}

## Existing Wiki Entries (for style reference)
```json
{example_entries}
```

---

Return the improved entry as a JSON object. For change_type "new_repo", return the full Project object. For "new_feature", return the updates object. For "status_update", return the status object. For "new_tech", return the technology deep dive object.

Return ONLY the JSON object, no markdown fences or explanation."""

    def __init__(self, model: str = "claude-opus-4-20250514",
                 temperature: float = 0.2, max_tokens: int = 8192):
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.api_key = os.environ.get("ANTHROPIC_API_KEY", "")

    def review(self, draft: dict, change_type: str, repo_name: str,
               repo_description: str, languages: str,
               example_entries: list[dict]) -> dict:
        """Review a draft entry and return the improved version."""
        if not self.api_key:
            logger.warning("ANTHROPIC_API_KEY not set. Skipping Opus review.")
            return draft

        sanitized_draft = _sanitize_for_prompt(draft, max_depth=3)
        sanitized_examples = _sanitize_for_prompt(example_entries[:2], max_depth=3)

        user_prompt = self.REVIEW_USER_TEMPLATE.format(
            draft_json=json.dumps(sanitized_draft, indent=2),
            change_type=change_type,
            repo_name=repo_name,
            repo_description=repo_description,
            languages=languages,
            example_entries=json.dumps(sanitized_examples, indent=2),
        )

        for attempt in range(3):
            try:
                result = self._call_anthropic(user_prompt)
                reviewed = self._parse_json_response(result)
                validation = _validate_learning_resource_urls(reviewed, change_type)
                if validation["invalid_urls"]:
                    logger.warning(
                        "Learning resource URL validation found issues for %s: %s",
                        repo_name,
                        ", ".join(validation["invalid_urls"]),
                    )
                    reviewed["_validation"] = validation
                logger.info(f"Opus review completed for {repo_name}")
                return reviewed
            except RateLimitError:
                wait = 10 * (attempt + 1)
                logger.warning(f"Anthropic rate limited. Waiting {wait}s...")
                time.sleep(wait)
            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse Opus response: {e}")
                if attempt < 2:
                    continue
                logger.warning("Returning original draft after review parse failure")
                return draft
            except Exception as e:
                logger.error(f"Opus review error: {e}")
                if attempt < 2:
                    time.sleep(2 ** attempt)
                    continue
                logger.warning("Returning original draft after review failure")
                return draft

        logger.warning("All Anthropic rate-limit retries exhausted for %s. Returning original draft.", repo_name)
        return draft

    def _call_anthropic(self, user_prompt: str) -> str:
        """Call the Anthropic Messages API directly."""
        url = "https://api.anthropic.com/v1/messages"
        payload = {
            "model": self.model,
            "max_tokens": self.max_tokens,
            "temperature": self.temperature,
            "system": self.REVIEW_SYSTEM_PROMPT,
            "messages": [
                {"role": "user", "content": user_prompt}
            ],
        }

        data = json.dumps(payload).encode()
        req = urllib.request.Request(
            url, data=data,
            headers={
                "Content-Type": "application/json",
                "x-api-key": self.api_key,
                "anthropic-version": "2023-06-01",
            },
            method="POST"
        )

        try:
            with urllib.request.urlopen(req, timeout=180) as resp:
                result = json.loads(resp.read().decode())
                # Extract text from the response
                content = result.get("content", [])
                for block in content:
                    if block.get("type") == "text":
                        return block["text"]
                raise RuntimeError("No text content in Anthropic response")
        except urllib.error.HTTPError as e:
            if e.code == 429:
                raise RateLimitError("Anthropic rate limit exceeded") from e
            body = e.read().decode() if e.readable() else str(e)
            raise RuntimeError(f"Anthropic API error {e.code}: {body}") from e

    def _parse_json_response(self, raw: str) -> dict:
        """Extract and parse JSON from response."""
        raw = raw.strip()
        if raw.startswith("```"):
            raw = re.sub(r'^```\w*\s*\n?', '', raw)
            raw = re.sub(r'\n?```\s*$', '', raw)
        return json.loads(raw)


def _sanitize_for_prompt(value, *, max_depth: int = 3, max_items: int = 8,
                         max_string_length: int = 600):
    """Create a truncated-safe object suitable for JSON serialization."""
    if max_depth < 0:
        return "[truncated]"

    if isinstance(value, dict):
        out = {}
        for idx, (k, v) in enumerate(value.items()):
            if idx >= max_items:
                out["__truncated_keys__"] = f"{len(value) - max_items} more keys"
                break
            out[k] = _sanitize_for_prompt(
                v,
                max_depth=max_depth - 1,
                max_items=max_items,
                max_string_length=max_string_length,
            )
        return out

    if isinstance(value, list):
        items = [
            _sanitize_for_prompt(
                item,
                max_depth=max_depth - 1,
                max_items=max_items,
                max_string_length=max_string_length,
            )
            for item in value[:max_items]
        ]
        if len(value) > max_items:
            items.append(f"[truncated {len(value) - max_items} more items]")
        return items

    if isinstance(value, str):
        if len(value) <= max_string_length:
            return value
        return value[:max_string_length] + "...[truncated]"

    return value


def _extract_urls(resource: str) -> list[str]:
    """Extract URLs from a markdown link or plain URL string."""
    match = re.search(r"\((https?://[^)]+)\)", resource)
    if match:
        return [match.group(1)]

    urls = re.findall(r"https?://[^\s]+", resource)
    return [u.rstrip('.,;') for u in urls]


def _validate_learning_resource_urls(reviewed: dict, change_type: str) -> dict:
    """Validate learning resource URLs for technology deep-dive entries."""
    result = {"checked_urls": [], "invalid_urls": [], "status": "skipped"}
    if change_type != "new_tech":
        return result

    value = reviewed.get("value", reviewed)
    learning_resources = value.get("learning_resources", []) if isinstance(value, dict) else []
    if not isinstance(learning_resources, list):
        return result

    result["status"] = "passed"
    for resource in learning_resources:
        if not isinstance(resource, str):
            continue
        urls = _extract_urls(resource)
        for url in urls:
            result["checked_urls"].append(url)
            parsed = urllib.parse.urlparse(url)
            if parsed.scheme not in {"http", "https"} or not parsed.netloc:
                result["invalid_urls"].append(f"{url} (invalid format)")
                result["status"] = "failed"
                continue

            req = urllib.request.Request(url, method="HEAD")
            try:
                with urllib.request.urlopen(req, timeout=10) as resp:
                    status_code = getattr(resp, "status", 200)
                    if status_code < 200 or status_code >= 300:
                        result["invalid_urls"].append(f"{url} (HTTP {status_code})")
                        result["status"] = "failed"
            except Exception:
                try:
                    fallback_req = urllib.request.Request(url, method="GET")
                    with urllib.request.urlopen(fallback_req, timeout=10) as resp:
                        status_code = getattr(resp, "status", 200)
                        if status_code < 200 or status_code >= 300:
                            result["invalid_urls"].append(f"{url} (HTTP {status_code})")
                            result["status"] = "failed"
                except Exception as e:
                    result["invalid_urls"].append(f"{url} ({e})")
                    result["status"] = "failed"

    return result


class RateLimitError(Exception):
    """Raised when an API rate limit is hit."""
    pass
